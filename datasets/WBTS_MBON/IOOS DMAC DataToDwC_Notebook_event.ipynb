{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligning Data to Darwin Core - Sampling Event with Measurement or Fact using Python\n",
    "Matt Biddle\n",
    "\n",
    "November 9, 2020\n",
    "\n",
    "# General information about this notebook\n",
    "This notebook was created for the IOOS DMAC Code Sprint Biological Data Session\n",
    "The data in this notebook were created specifically as an example and meant solely to be\n",
    "illustrative of the process for aligning data to the biological data standard - Darwin Core.\n",
    "These data should not be considered actually occurrences of species and any measurements\n",
    "are also contrived. This notebook is meant to provide a step by step process for taking\n",
    "original data and aligning it to Darwin Core\n",
    "\n",
    "This notebook is a python implementation of the R notebook [IOOS_DMAC_DataToDWC_Notebook_event.R](https://github.com/ioos/bio_data_guide/blob/master/Standardizing%20Marine%20Biological%20Data/datasets/example_script_with_fake_data/IOOS_DMAC_DataToDwC_Notebook_event.R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyworms # pip install git+git://github.com/iobis/pyworms.git\n",
    "import numpy as np\n",
    "import uuid\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the raw data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.neracoos.org/erddap/tabledap/WBTS_CFIN_2005_2017.csv\"\n",
    "df = pd.read_csv(url, header=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to to decide if we will provide an occurrence only version of the data or\n",
    "a sampling event with measurement or facts version of the data. Occurrence only is easier\n",
    "to create. It's only one file to produce. However, several pieces of information will be\n",
    "left out if we choose that option. If we choose to do sampling event with measurement or\n",
    "fact we'll be able to capture all of the data in the file creating a lossless version.\n",
    "Here we decide to use the sampling event option to include as much information as we can.\n",
    "\n",
    "First let's create the eventID and occurrenceID in the original file so that information\n",
    "can be reused for all necessary files down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['eventID'] = df[['Cruise_Identification_Tag', 'Station_ID']].apply(lambda x: '_'.join(x.astype(str)), axis=1)\n",
    "df['occurrenceID'] = uuid.uuid4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to create three separate files to comply with the sampling event format.\n",
    "We'll start with the event file but we only need to include the columns that are relevant\n",
    "to the event file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = df[['time', 'latitude', 'longitude', 'Station_ID', 'CRUISE_ID', 'NET_DEPTH', 'STATION_DEPTH', 'eventID']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to rename any columns of data that match directly to Darwin Core. We know\n",
    "this based on our crosswalk spreadsheet CrosswalkToDarwinCore.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event['decimalLatitude'] = event['latitude']\n",
    "event['decimalLongitude'] = event['longitude']\n",
    "event['minimumDepthInMeters'] = event['NET_DEPTH']\n",
    "event['maximumDepthInMeters'] = event['NET_DEPTH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also have to add any missing required fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event['basisOfRecord'] = 'HumanObservation'\n",
    "\n",
    "#This is a guess\n",
    "event['geodeticDatum'] = 'EPSG:4326 WGS84'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll remove any columns that we no longer need to clean things up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event.drop(\n",
    "    columns=['latitude', 'longitude', 'Station_ID', 'CRUISE_ID', 'NET_DEPTH'],\n",
    "    inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have too many repeating rows of information. We can pare this down using eventID which\n",
    "is a unique identifier for each sampling event in the data- which is six, three transects\n",
    "per site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event.drop_duplicates(\n",
    "    subset='eventID',\n",
    "    inplace=True)\n",
    "\n",
    "event.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we write out the event file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event.to_csv(\n",
    "    'results/WBTS_CFIN_2005_2017_event_frompy.csv',\n",
    "    header=True,\n",
    "    index=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occurrence file\n",
    "Next we need to create the occurrence file. We start by examining the structure (columns) of the source data. The goal here is to assess what kind of conversion (if any) will be necessary for Darwin Core alignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the `Calanus_finmarchicus` columns need to be converted into a more suitable format. We need to iterate through the existing data row by row - the goal is to create five new columns: `scientificName`, `lifeStage`, `sex`, `occuranceStatus`, & `individualCount`.\n",
    "\n",
    "We start by isolating the records that have valid data. We define the columns we want to check against as `target_data_columns`, and then create a new dataframe `calanus_records` by retaining only records where at least one of the columns has a value of NOT `0` AND NOT `NaN`.\n",
    "\n",
    "We also drop the second row, which contains unit information to avoid confusing the parser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data_columns = ['Calanus_finmarchicus_N',\n",
    "                       'Calanus_finmarchicus_CI',\n",
    "                       'Calanus_finmarchicus_CII',\n",
    "                       'Calanus_finmarchicus_CIII',\n",
    "                       'Calanus_finmarchicus_CIV',\n",
    "                       'Calanus_finmarchicus_CV',\n",
    "                       'Calanus_finmarchicus_F',\n",
    "                       'Calanus_finmarchicus_M']\n",
    "\n",
    "calanus_records = df.loc[(pd.notna(df[target_data_columns]) & (df[target_data_columns] != 0)).all(1)]\n",
    "\n",
    "# drop units row from calanus records\n",
    "calanus_records = calanus_records.iloc[1:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge is that, in its current form, each row actually represents between 0 and 8 discrete occurances. This isn't suitable for Darwin Core, so we need to read each row, and then split its data into new records, each representing an occurance event. This is a little tricky, so we'll create a helper method `enumerate_row` which takes a row (a `pandas.Series` object in practice) and makes the appropriate transformations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enumerate_row(row, field):\n",
    "    # expands rows which contain multiple observations into discrete records\n",
    "    row_data = row[1]\n",
    "    calanus_count = row_data[field]\n",
    "\n",
    "    # convert to dict so we can mutate\n",
    "    enumerated_row = row_data.to_dict()\n",
    "\n",
    "    split_species = field.rsplit('_', 1)\n",
    "    scientific_name = split_species[0].replace('_', ' ')\n",
    "    life_stage = split_species[1]\n",
    "\n",
    "    # add count of specified species as a new column\n",
    "    enumerated_row['individualCount'] = calanus_count\n",
    "    enumerated_row['scientificName'] = scientific_name\n",
    "    # we're only processing records with occurances\n",
    "    enumerated_row['occurrenceStatus'] = 'present'\n",
    "\n",
    "    life_stage = field.rsplit('_', 1)[1]\n",
    "    enumerated_row['lifeStage'] = life_stage if life_stage != 'F' and life_stage != 'M' else 'adult'\n",
    "\n",
    "    if life_stage == 'F':\n",
    "        enumerated_row['sex'] = 'female'\n",
    "    elif life_stage == 'M':\n",
    "        enumerated_row['sex'] = 'male'\n",
    "    else:\n",
    "        enumerated_row['sex'] = 'NA'\n",
    "\n",
    "    return enumerated_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to loop through the target data. The top-level control variable is the list of the columns we wish to enumerate, so we will look for each target column in each row of the dataset. \n",
    "\n",
    "*note*: This operation could easily become costly depending on the number of rows and target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enumerated_rows = []\n",
    "# loop through target column list, for each, select all records (via loc) where a given column has a value of >0\n",
    "for field in target_data_columns:\n",
    "\n",
    "    # returns df with all records where there is an occurance of the given calanus\n",
    "    current_df = calanus_records.loc[pd.to_numeric(calanus_records[field]) > 0]\n",
    "\n",
    "    # now enumerate each input row, extracting the values\n",
    "    for row in current_df.iterrows():\n",
    "\n",
    "        flipped_row = enumerate_row(row, field)\n",
    "\n",
    "        # delete other calanus records from flipped row\n",
    "        for k in target_data_columns:\n",
    "            flipped_row.pop(k, None)\n",
    "\n",
    "        enumerated_rows.append(flipped_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit of clean up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now convert the list of dicts into a dataframe\n",
    "output_frame = pd.DataFrame.from_dict(enumerated_rows)\n",
    "\n",
    "# sort by time, ascending\n",
    "output_frame.sort_values(by='time', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data should be in a more suitable fromat, so we can proceed. \n",
    "\n",
    "We start by creating a new occurrence data frame with the relevant fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence = output_frame[['scientificName', 'eventID', 'occurrenceID', 'individualCount', 'occurrenceStatus', 'lifeStage', 'sex']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxonomic Name Matching\n",
    "A requirement for OBIS is that all scientific names match to the World Register of\n",
    "Marine Species (WoRMS) and a scientificNameID is included. A scientificNameID looks\n",
    "like this \"urn:lsid:marinespecies.org:taxname:275730\" with the last digits after\n",
    "the colon being the WoRMS aphia ID. We'll need to go out to WoRMS to grab this\n",
    "information.\n",
    "\n",
    "Create a lookup table of unique scientific names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lut_worms = pd.DataFrame(\n",
    "    columns=['scientificName'],\n",
    "    data=occurrence['scientificName'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the columns that we can grab information from WoRMS including the required scientificNameID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['acceptedname', 'acceptedID', 'scientificNameID', 'kingdom', 'phylum',\n",
    "           'class', 'order', 'family', 'genus', 'scientificNameAuthorship', 'taxonRank']\n",
    "\n",
    "for head in headers:\n",
    "    lut_worms[head] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taxonomic lookup using the library [pyworms](https://github.com/iobis/pyworms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in lut_worms.iterrows():\n",
    "    print('Searching for scientific name = %s' % row['scientificName'])\n",
    "    resp = pyworms.aphiaRecordsByMatchNames(row['scientificName'])[0][0]\n",
    "    lut_worms.loc[index, 'acceptedname'] = resp['valid_name']\n",
    "    lut_worms.loc[index, 'acceptedID'] = resp['valid_AphiaID']\n",
    "    lut_worms.loc[index, 'scientificNameID'] = resp['lsid']\n",
    "    lut_worms.loc[index, 'kingdom'] = resp['kingdom']\n",
    "    lut_worms.loc[index, 'phylum'] = resp['phylum']\n",
    "    lut_worms.loc[index, 'class'] = resp['class']\n",
    "    lut_worms.loc[index, 'order'] = resp['order']\n",
    "    lut_worms.loc[index, 'family'] = resp['family']\n",
    "    lut_worms.loc[index, 'genus'] = resp['genus']\n",
    "    lut_worms.loc[index, 'scientificNameAuthorship'] = resp['authority']\n",
    "    lut_worms.loc[index, 'taxonRank'] = resp['rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the lookup table of unique scientific names back with the occurrence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence = pd.merge(occurrence, lut_worms, how='left', on='scientificName')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to remove any unnecessary columns to clean up the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence.drop(\n",
    "    columns=['scientific name', 'percent cover'],\n",
    "    inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick look at what we have before we write out the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the columns on scientificName\n",
    "occurrence.sort_values('scientificName', inplace=True)\n",
    "# reorganize column order to be consistent with R example:\n",
    "columns = [\"scientificName\",\"eventID\",\"occurrenceID\",\"occurrenceStatus\",\"acceptedname\",\"acceptedID\",\n",
    "           \"scientificNameID\",\"kingdom\",\"phylum\",\"class\",\"order\",\"family\",\"genus\",\"scientificNameAuthorship\",\n",
    "           \"taxonRank\"]\n",
    "\n",
    "occurrence.to_csv(\n",
    "    \"results/WBTS_CFIN_2005_2017_occurrence_frompy.csv\",\n",
    "    header=True,\n",
    "    index=False,\n",
    "    quoting=csv.QUOTE_ALL,\n",
    "    columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " All done with occurrence!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measurement Or Fact\n",
    "The last file we need to create is the measurement or fact file. For this we need to\n",
    "combine all of the measurements or facts that we want to include making sure to include\n",
    "IDs from the BODC NERC vocabulary where possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll manually map the remaining variables to the BODC NERC vocabulary when possible. For now we're mapping the following metadata for each field:\n",
    "\n",
    "1. uri -> URL of the concept page on the NERC VOcabulary Server (NVS)\n",
    "2. unit\n",
    "3. unitId -> URL of the unit ID page on NVS\n",
    "4. accuracy \n",
    "5. type -> measurement type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "\n",
    "vocab_url_prefix = 'http://vocab.nerc.ac.uk/collection/'\n",
    "\n",
    "column_mappings = {\n",
    "    'Cast': {'uri': 'P25/current/VOL/', 'unit': 'm2', 'unitID': 'P06/current/UPAA/', 'accuracy': '', 'type': ''},\n",
    "    'Net_Type': {'uri': 'P25/current/VOL/', 'unit': 'm2', 'unitID': 'P06/current/UPAA/', 'accuracy': '', 'type': ''},\n",
    "    'Mesh_Size': {'uri': 'P25/current/VOL/', 'unit': 'm2', 'unitID': 'P06/current/UPAA/', 'accuracy': '', 'type': ''},\n",
    "    'NET_DEPTH': {'uri': 'P25/current/VOL/', 'unit': 'm2', 'unitID': 'P06/current/UPAA/', 'accuracy': '', 'type': ''},\n",
    "    'STATION_DEPTH': {'uri': 'P25/current/VOL/', 'unit': 'm2', 'unitID': 'P06/current/UPAA/', 'accuracy': '', 'type': ''},\n",
    "    'Plankton_Net_Area': {'uri': 'P25/current/VOL/', 'unit': 'm2', 'unitID': 'P06/current/UPAA/', 'accuracy': '', 'type': ''},\n",
    "    'Volume_Filtered': {'uri': 'P25/current/VOL/', 'unit': 'm2', 'unitID': 'P06/current/UPAA/', 'accuracy': '', 'type': ''},\n",
    "    'Sample_Split': {'uri': 'P25/current/VOL/', 'unit': 'm2', 'unitID': 'P06/current/UPAA/', 'accuracy': '', 'type': ''},\n",
    "    'Sample_Dry_Weight': {'uri': 'P25/current/VOL/', 'unit': 'm2', 'unitID': 'P06/current/UPAA/', 'accuracy': '', 'type': ''},\n",
    "    'DW_G_M_2': {'uri': 'P25/current/VOL/', 'unit': 'm2', 'unitID': 'P06/current/UPAA/', 'accuracy': '', 'type': ''},\n",
    "    'Dilution_Factor': {'uri': 'P25/current/VOL/', 'unit': 'm2', 'unitID': 'P06/current/UPAA/', 'accuracy': '', 'type': ''},\n",
    "    'TOTAL_DILFACTOR_CFIN': {'uri': 'P25/current/VOL/', 'unit': 'm2', 'unitID': 'P06/current/UPAA/', 'accuracy': '', 'type': ''},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we'll loop through the mapping list and transform as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_to_concat = []\n",
    "\n",
    "for current_field in column_mappings:\n",
    "\n",
    "    current_mapping = column_mappings.get(current_field)\n",
    "    \n",
    "    current_df = df[['eventID', current_field, 'time']].copy()\n",
    "\n",
    "    current_df['occurrenceID'] = ''\n",
    "    current_df['measurementType'] = current_mapping.get('type')\n",
    "    current_df['measurementTypeID'] = vocab_url_prefix + current_mapping.get('uri')\n",
    "    current_df['measurementValue'] = current_df[current_field]\n",
    "    current_df['measurementUnit'] = current_mapping.get('unit')\n",
    "    current_df['measurementUnitID'] = vocab_url_prefix + current_mapping.get('unitID')\n",
    "    current_df['measurementAccuracy'] = current_mapping.get('accuracy')\n",
    "    current_df['measurementDeterminedDate'] = current_df['time']\n",
    "    current_df['measurementMethod'] = ''\n",
    "    current_df.drop(\n",
    "        columns=[current_field, 'time'],\n",
    "        inplace=True)\n",
    "    \n",
    "    frames_to_concat.append(current_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all measurements or facts together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurementorfact = pd.concat(frames_to_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurementorfact.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write measurement or fact file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurementorfact.to_csv('results/WBTS_CFIN_2005_2017_mof_frompy.csv',\n",
    "                         index=False,\n",
    "                         header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
