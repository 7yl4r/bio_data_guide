[["index.html", "Standardizing-Marine-Biological-Data Preface", " Standardizing-Marine-Biological-Data Standardizing Marine Biological Data Working Group 2021-11-04 Preface Biological data structures, definitions, measurements, and linkages are neccessarily as diverse as the systems they represent. This presents a real challenge when integrating data across biological research domains such as ecology, oceanography, fisheries, and climate sciences. "],["intro.html", "Chapter 1 Introduction 1.1 Data Structures 1.2 Ontologies 1.3 Controlled Vocabularies 1.4 Technologies 1.5 Notes on Integrating OBIS, Darwin Core as it relates to OOS’s 1.6 Metadata 1.7 Data QC", " Chapter 1 Introduction The world of standardizing marine biological data can seem complex for the naive oceanographer, biologist, scientist, or programmer. Transforming and integrating data is about combining the right standards for your desired interoperability with other data types. For example, interoperating fish biology measurements with climate level variables. There are a few concepts necessary to make this possible such as standard data structures, controlled vocabularies and knowledge representations, along with metadata standards to facilitate data discovery. This will permit the inclusion of more data and broader access to better ecosystem based models. Many scientific domains data handling practices are currently being reshaped in light of recent advances in computing power, technology, and data science. 1.1 Data Structures The OBIS-ENV Darwin Core Archive Data Structure. OBIS manual 1.2 Ontologies An ontology is a classification system for establishing a hierarchically related set of concepts. Concepts are often terms from controlled vocabularies. From Marine Metadata: # TODO: add link “Ontologies can include all of the following, but are not required to include them, depending on which perspective from above you adhere to: Classes (general things, types of things) Instances (individual things) Relationships among things Properties of things Functions, processes, constraints, and rules relating to things” TODO: Research Unified Modelling Language? Environment Ontology (EnvO) EnvO is a community ontology for the concise, controlled description of environments. 1.3 Controlled Vocabularies There are a number of controlled vocabularies that are used to describe parameters commonly used in specific research domains. This allows for greater interoperability of data sets within the domain, and ideally between domains. Here, we strive to document a number of relevant examples. Climate and Format (CF) Standard Names The purpose of the standard_name attribute is to provide a succinct and distinguishing description of a variable, in a way that encourages interoperability. These terms are typically for physical observations, however, there have been advancements in aligning biological taxa into the CF standard names (see here). NERC Vocabulary Server The NVS gives access to standardised and hierarchically-organized vocabularies. Device categories using the SeaDataNet device categories Device make/model using the SeaVoX Device Catalogue Platform categories using SeaVoX Platform Categories Platform instances using the ICES Platform Codes Unit of measure GCMD Keywords (NASA) Geographic Domain/Features of Interest GeoLink base ontology was part of the EarthCube GeoLink Project Note: To describe a measurement or fact of a biological specimen that conforms to Darwin Core standards, it’s necessary to use the ‘Biological entity described elsewhere’ method rather than taxon specific. 1.3.1 Taxonomy The World Registry of Marine Species (WoRMS) The aim of a World Register of Marine Species (WoRMS) is to provide an authoritative and comprehensive list of names of marine organisms, including information on synonymy. While the highest priority goes to valid names, other names in use are included so that this register can serve as a guide to interpret taxonomic literature. 1.3.2 Resources 1.3.3 Oceanography Biological and Chemcial Oceanography Data Management Office Marine metadata interoperability vocab resources 1.3.4 Biology BioPortal Ecosystem Ontology 1.3.5 NERC Search Interfaces SeaDatanet Common Vocab Search Interface: SeaDataNet Common Vocabularies: SeaDataNet Vocab Library Measurement Types in OBIS 1.3.6 Geosciences UDUNITSare more common unit measurements in geosciences 1.3.7 Eco/EnvO Environment Ontology including genomics. 1.3.8 Wild Cards Question: Not sure use case for this. P01 Biological Entity Parameter Code Builder 1.4 Technologies 1.4.1 ERDDAP ERDDAP can be thought of as a data server. It provides ‘easier access to scientific data’ by providing a consistent interface that aggregates many disparate data sources. It does this by providing translation services between many common file types for gridded arrarys (‘net CDF’ files) and tabular data (spreadsheets). Data access is also made easier because it unifies different types of data servers and access protocols. Here is a basic erddap installation that walks you through how to load a data set. 1.5 Notes on Integrating OBIS, Darwin Core as it relates to OOS’s 1.6 Metadata OBIS uses the GBIF EML profile (version 1.1). In case data providers use ISO19115/ISO19139, there is a mapping available here: http://rs.gbif.org/schema/eml-gbif-profile/1.1/eml2iso19139.xsl This will be important for integrating OBIS datasets to other CIOOS and IOOS metadata profiles. 1.7 Data QC There are a number of tools available to check the quality of data or check your data format against the expected standard. OBIS Datatools shows some great R packages for this. 1.7.1 Compliance Checking LifeWatch Belgium provides a number of tools to check your data against. Specifically you can test OBIS data format and see a map of your sample locations to check if they are on land. See http://www.lifewatch.be/data-services/ There’s also the GBIF data validator which allows anyone with a GBIF-relevant dataset to receive a report on the syntactical correctness and the validity of the content contained within the dataset. 1.7.2 Semantic Web and Darwin Core Lessons learned from adapting the Darwin Core vocabulary standard for use in RDF 1.7.3 Resource Description Framework Darwin Core Resource Description Framework Guide "],["applications.html", "Chapter 2 Applications 2.1 Salmon Ocean Ecology Data 2.2 Hakai Seagrass", " Chapter 2 Applications Some significant applications are demonstrated in this chapter. 2.1 Salmon Ocean Ecology Data 2.1.1 Intro One of the goals of the Hakai Institute and the Canadian Integrated Ocean Observing System (CIOOS) is to facilitate Open Science and FAIR (findable, accessible, interoperable, reusable) ecological and oceanographic data. In a concerted effort to adopt or establish how best to do that, several Hakai and CIOOS staff attended an International Ocean Observing System (IOOS) Code Sprint in Ann Arbour, Michigan between October 7–11, 2019, to discuss how to implement FAIR data principles for biological data collected in the marine environment. The Darwin Core is a highly structured data format that standardizes data table relations, vocabularies, and defines field names. The Darwin Core defines three table types: event, occurrence, and measurementOrFact. This intuitively captures the way most ecologists conduct their research. Typically, a survey (event) is conducted and measurements, counts, or observations (collectively measurementOrFacts) are made regarding a specific habitat or species (occurrence). In the following script I demonstrate how I go about converting a subset of the data collected from the Hakai Institute Juvenile Salmon Program and discuss challenges, solutions, pros and cons, and when and what’s worthwhile to convert to Darwin Core. The conversion of a dataset to Darwin Core is much easier if your data are already tidy (normalized) in which you represent your data in separate tables that reflect the hierarchical and related nature of your observations. If your data are not already in a consistent and structured format, the conversion would likely be very arduous and not intuitive. 2.1.2 event The first step is to consider what you will define as an event in your data set. I defined the capture of fish using a purse seine net as the event. Therefore, each row in the event table is one deployment of a seine net and is assigned a unique eventID. My process for conversion was to make a new table called event and map the standard Darwin Core column names to pre-existing columns that serve the same purpose in my original seine_data table and populate the other required fields. #TODO: Include abiotic measurements (YSI temp and salinity from 0 and 1 m) to hang off eventID in the eMoF table event &lt;- tibble(datasetName = &quot;Hakai Institute Juvenile Salmon Program&quot;, eventID = survey_seines$seine_id, eventDate = date(survey_seines$survey_date), eventTime = paste0(survey_seines$set_time, &quot;-0700&quot;), eventRemarks = paste3(survey_seines$survey_comments, survey_seines$seine_comments), decimalLatitude = survey_seines$lat, decimalLongitude = survey_seines$long, locationID = survey_seines$site_id, coordinatePrecision = 0.00001, coordinateUncertaintyInMeters = 10, country = &quot;Canada&quot;, countryCode = &quot;CA&quot;, stateProvince = &quot;British Columbia&quot;, habitat = &quot;Nearshore marine&quot;, geodeticDatum = &quot;EPSG:4326 WGS84&quot;, minimumDepthInMeters = 0, maximumDepthInMeters = 9, # seine depth is 9 m samplingProtocol = &quot;http://dx.doi.org/10.21966/1.566666&quot;, # This is the DOI for the Hakai Salmon Data Package that contains the smnpling protocol, as well as the complete data package language = &quot;en&quot;, license = &quot;http://creativecommons.org/licenses/by/4.0/legalcode&quot;, bibliographicCitation = &quot;Johnson, B.T., J.C.L. Gan, S.C. Godwin, M. Krkosek, B.P.V. Hunt. 2020. Hakai Juvenile Salmon Program Time Series. Hakai Institute, Quadra Island Ecological Observatory, Heriot Bay, British Columbia, Canada. v#.#.#, http://dx.doi.org/10.21966/1.566666&quot;, references = &quot;https://github.com/HakaiInstitute/jsp-data&quot;, institutionID = &quot;https://www.gbif.org/publisher/55897143-3f69-42f1-810d-ae94b55fde24, https://oceanexpert.org/institution/20121, https://edmo.seadatanet.org/report/5148&quot;, institutionCode = &quot;Hakai&quot; ) 2.1.3 occurrence Next you’ll want to determine what constitutes an occurrence for your data set. Because each event captures fish, I consider each fish to be an occurrence. Therefore, the unit of observation (each row) in the occurrence table is a fish. To link each occurrence to an event you need to include the eventID column for every occurrence so that you know what seine (event) each fish (occurrence) came from. You must also provide a globally unique identifier for each occurrence. I already have a locally unique identifier for each fish in the original fish_data table called ufn. To make it globally unique I pre-pend the organization and research program metadata to the ufn column. Not every fish is actually collected and given a Universal Fish Number (UFN) in our fish data tables, so in our field data sheets we record the total number of fish captured and the total number retained. So to get an occurrence row for every fish captured I create a row for every fish caught (minus the number taken) and create a generic numeric id (ie hakai-jsp-1) in one table and then join that to the fish table that includes a row for every fish retained that already has a UFN. ## make table long first seines_total_long &lt;- survey_seines %&gt;% select(seine_id, so_total, pi_total, cu_total, co_total, he_total, ck_total) %&gt;% pivot_longer(-seine_id, names_to = &quot;scientificName&quot;, values_to = &quot;n&quot;) seines_total_long$scientificName &lt;- recode(seines_total_long$scientificName, so_total = &quot;Oncorhynchus nerka&quot;, pi_total = &quot;Oncorhynchus gorbuscha&quot;, cu_total = &quot;Oncorhynchus keta&quot;, co_total = &quot;Oncorhynchus kisutch&quot;, ck_total = &quot;Oncorhynchus tshawytscha&quot;, he_total = &quot;Clupea pallasii&quot;) seines_taken_long &lt;- survey_seines %&gt;% select(seine_id, so_taken, pi_taken, cu_taken, co_taken, he_taken, ck_taken) %&gt;% pivot_longer(-seine_id, names_to = &quot;scientificName&quot;, values_to = &quot;n_taken&quot;) seines_taken_long$scientificName &lt;- recode(seines_taken_long$scientificName, so_taken = &quot;Oncorhynchus nerka&quot;, pi_taken = &quot;Oncorhynchus gorbuscha&quot;, cu_taken = &quot;Oncorhynchus keta&quot;, co_taken = &quot;Oncorhynchus kisutch&quot;, ck_taken = &quot;Oncorhynchus tshawytscha&quot;, he_taken = &quot;Clupea pallasii&quot;) ## remove records that have already been assigned an ID because they were actually retained seines_long &lt;- full_join(seines_total_long, seines_taken_long, by = c(&quot;seine_id&quot;, &quot;scientificName&quot;)) %&gt;% drop_na() %&gt;% mutate(n_not_taken = n - n_taken) %&gt;% #so_total includes the number taken so I subtract n_taken to get n_not_taken select(-n_taken, -n) %&gt;% filter(n_not_taken &gt; 0) all_fish_not_retained &lt;- seines_long[rep(seq.int(1, nrow(seines_long)), seines_long$n_not_taken), 1:3] %&gt;% select(-n_not_taken) %&gt;% mutate(prefix = &quot;hakai-jsp-&quot;, suffix = 1:nrow(.), occurrenceID = paste0(prefix, suffix) ) %&gt;% select(-prefix, -suffix) # # Change species names to full Scientific names latin &lt;- fct_recode(fish_data$species, &quot;Oncorhynchus nerka&quot; = &quot;SO&quot;, &quot;Oncorhynchus gorbuscha&quot; = &quot;PI&quot;, &quot;Oncorhynchus keta&quot; = &quot;CU&quot;, &quot;Oncorhynchus kisutch&quot; = &quot;CO&quot;, &quot;Clupea pallasii&quot; = &quot;HE&quot;, &quot;Oncorhynchus tshawytscha&quot; = &quot;CK&quot;) %&gt;% as.character() fish_retained_data &lt;- fish_data %&gt;% mutate(scientificName = latin) %&gt;% select(-species) %&gt;% mutate(prefix = &quot;hakai-jsp-&quot;, occurrenceID = paste0(prefix, ufn)) %&gt;% select(seine_id, scientificName, occurrenceID) occurrence &lt;- bind_rows(all_fish_not_retained, fish_retained_data) %&gt;% rename(eventID = seine_id) %&gt;% # rename = dplyr::rename; vs plyr::rename mutate(`Life stage` = &quot;juvenile&quot;) unique_taxa &lt;- unique(occurrence$scientificName) worms_names &lt;- wm_records_names(unique_taxa) # library(worrms) df_worms_names &lt;- bind_rows(worms_names) %&gt;% select(scientificName = scientificname, scientificNameAuthorship = authority, taxonRank = rank, scientificNameID = lsid ) #include bycatch species unique_bycatch &lt;- unique(bycatch$scientificName) %&gt;% glimpse() by_worms_names &lt;- wm_records_names(unique_bycatch) %&gt;% bind_rows() %&gt;% select(scientificName = scientificname, scientificNameAuthorship = authority, taxonRank = rank, scientificNameID = lsid ) bycatch_occurrence &lt;- bycatch %&gt;% select(eventID = seine_id, occurrenceID, scientificName, `Life stage` = bm_ageclass) %&gt;% filter(scientificName != &quot;unknown&quot;) bycatch_occurrence$`Life stage`[bycatch_occurrence$`Life stage` == &quot;J&quot;] &lt;- &quot;juvenile&quot; bycatch_occurrence$`Life stage`[bycatch_occurrence$`Life stage` == &quot;A&quot;] &lt;- &quot;adult&quot; bycatch_occurrence$`Life stage`[bycatch_occurrence$`Life stage` == &quot;Y&quot;] &lt;- &quot;Young of year&quot; combined_worms_names &lt;- bind_rows(by_worms_names, df_worms_names) %&gt;% distinct(scientificName, .keep_all = TRUE) occurrence &lt;- bind_rows(bycatch_occurrence, occurrence) occurrence &lt;- left_join(occurrence, combined_worms_names) %&gt;% mutate(basisOfRecord = &quot;HumanObservation&quot;, occurrenceStatus = &quot;present&quot;) write_csv(occurrence,&quot;../datasets/hakai_salmon_data/raw_data/occurrence.csv&quot;) # here::here(&quot;..&quot;, &quot;datasets&quot;, &quot;hakai_salmon_data&quot;, &quot;raw_data&quot;, &quot;occurrence.csv&quot;)) # This removes events that didn&#39;t result in any occurrences event &lt;- dplyr::semi_join(event, occurrence, by = &#39;eventID&#39;) %&gt;% mutate(coordinateUncertaintyInMeters = ifelse(is.na(decimalLatitude), 1852, coordinateUncertaintyInMeters)) simple_sites &lt;- sites %&gt;% select(site_id, ocgy_std_lat, ocgy_std_lon) event &lt;- dplyr::left_join(event, simple_sites, by = c(&quot;locationID&quot; = &quot;site_id&quot;)) %&gt;% mutate(decimalLatitude = coalesce(decimalLatitude, ocgy_std_lat), decimalLongitude = coalesce(decimalLongitude, ocgy_std_lon)) %&gt;% select(-c(ocgy_std_lat, ocgy_std_lon)) write_csv(event,&quot;../datasets/hakai_salmon_data/raw_data/event.csv&quot;) # here::here(&quot;..&quot;, &quot;datasets&quot;, &quot;hakai_salmon_data&quot;, &quot;raw_data&quot;, &quot;event.csv&quot;)) 2.1.4 measurementOrFact To convert all your measurements or facts from your normal format to Darwin Core you essentially need to put all your measurements into one column called measurementType and a corresponding column called MeasurementValue. This standardizes the column names are in the measurementOrFact table. There are a number of predefined measurementTypes listed on the NERC database that should be used where possible. I found it difficult to navigate this page to find the correct measurementType. Here I convert length, and weight measurements that relate to an event and an occurrence and call those measurementTypes as length and weight. mof_types &lt;- read_csv(&quot;https://raw.githubusercontent.com/HakaiInstitute/jsp-data/master/OBIS_data/mof_type_units_id.csv&quot;) fish_data$weight &lt;- coalesce(fish_data$weight, fish_data$weight_field) fish_data$fork_length &lt;- coalesce(fish_data$fork_length, fish_data$fork_length_field) fish_data$`Life stage` &lt;- &quot;juvenile&quot; measurementOrFact &lt;- fish_data %&gt;% mutate(occurrenceID = paste0(&quot;hakai-jsp-&quot;, ufn)) %&gt;% select(occurrenceID, eventID = seine_id, &quot;Length (fork length)&quot; = fork_length, &quot;Standard length&quot; = standard_length, &quot;Weight&quot; = weight, `Life stage`) %&gt;% pivot_longer(`Length (fork length)`:`Life stage`, names_to = &quot;measurementType&quot;, values_to = &quot;measurementValue&quot;, values_transform = list(measurementValue = as.character)) %&gt;% filter(measurementValue != &quot;NA&quot;) %&gt;% left_join(mof_types,by = c(&quot;measurementType&quot;)) %&gt;% mutate(measurementValueID = case_when(measurementValue == &quot;juvenile&quot; ~ &quot;http://vocab.nerc.ac.uk/collection/S11/current/S1127/&quot;), measurementID = paste(eventID, measurementType, occurrenceID, sep = &quot;-&quot;)) write_csv(measurementOrFact,&quot;../datasets/hakai_salmon_data/raw_data/extendedMeasurementOrFact.csv&quot;) # here::here(&quot;..&quot;, &quot;datasets&quot;, &quot;hakai_salmon_data&quot;, &quot;raw_data&quot;, &quot;extendedMeasurementOrFact.csv&quot;)) #Check that every eventID in Occurrence occurs in event table no_keys &lt;- dm(event, occurrence, measurementOrFact) only_pk &lt;- no_keys %&gt;% dm_add_pk(event, eventID) %&gt;% dm_add_pk(occurrence, occurrenceID) %&gt;% dm_add_pk(measurementOrFact, measurementID) dm_examine_constraints(only_pk) model &lt;- only_pk %&gt;% dm_add_fk(occurrence, eventID, event) %&gt;% dm_add_fk(measurementOrFact, occurrenceID, occurrence) dm_examine_constraints(model) #TODO: Fix bookdown issues so that dm_draw shows data model html output. Perhaps add `always_allow_html: true` to yaml front matter # dm_draw(model, view_type = &quot;all&quot;) 2.2 Hakai Seagrass By: ZL Monteith, Hakai Institute 2.2.1 Setup This section clears the workspace, checks the working directory, and installs packages (if required) and loads packages, and loads necessary datasets # The following command will remove all objects** for a fresh start. Make # sure any objects you want to keep are saved before running! rm(list = ls()) # Check working directory; set if necessary so document will compile # properly getwd() # Install packages; uncomment and run if packages not already installed # install.packages(c(&quot;tidyverse&quot;, &quot;uuid&quot;)) # Load packages lapply(c(&quot;tidyverse&quot;, &quot;lubridate&quot;, &quot;magrittr&quot;, &quot;worrms&quot;), library, character.only = TRUE) 2.2.1.1 Load Data First load the seagrass density survey data, set variable classes, and have a quick look # Load density data seagrassDensity &lt;- read.csv(&quot;https://raw.githubusercontent.com/ioos/bio_data_guide/main/datasets/hakai_seagrass_data/raw_data/seagrass_density_survey.csv&quot;, colClass = &quot;character&quot;) %&gt;% mutate(date = ymd(date), depth = as.numeric(depth), transect_dist = factor(transect_dist), collected_start = ymd_hms(collected_start), collected_end = ymd_hms(collected_end), density = as.numeric(density), density_msq = as.numeric(density_msq), canopy_height_cm = as.numeric(canopy_height_cm), flowering_shoots = as.numeric(flowering_shoots)) %T&gt;% glimpse() Next, load the habitat survey data, and same as above, set variable classes as necessary, and have a quick look. # load habitat data, set variable classes, have a quick look seagrassHabitat &lt;- read.csv(&quot;https://raw.githubusercontent.com/ioos/bio_data_guide/main/datasets/hakai_seagrass_data/raw_data/seagrass_habitat_survey.csv&quot;, colClasses = &quot;character&quot;) %&gt;% mutate(date = ymd(date), depth = as.numeric(depth), hakai_id = str_pad(hakai_id, 5, pad = &quot;0&quot;), transect_dist = factor(transect_dist), collected_start = ymd_hms(collected_start), collected_end = ymd_hms(collected_end)) %T&gt;% glimpse() Finally, load coordinate data for surveys, and subset necessary variables coordinates &lt;- read.csv(&quot;https://raw.githubusercontent.com/ioos/bio_data_guide/main/datasets/hakai_seagrass_data/raw_data/seagrassCoordinates.csv&quot;, colClass = c(&quot;Point.Name&quot; = &quot;character&quot;)) %&gt;% select(Point.Name, Decimal.Lat, Decimal.Long) %T&gt;% glimpse() 2.2.1.2 Merge Datasets Now all the datasets have been loaded, and briefly formatted, we’ll join together the habitat and density surveys, and the coordinates for these. The seagrass density surveys collect data at discrete points (ie. 5 metres) along the transects, while the habitat surveys collect data over sections (ie. 0 - 5 metres) along the transects. In order to fit these two surveys together, we’ll narrow the habitat surveys from a range to a point so the locations will match. Based on how the habitat data is collected, the point the habitat survey is applied to will be the distance at the end of the swath (ie. 10-15m will become 15m). To account for no preceeding distance, the 0m distance will use the 0-5m section of the survey. First, well make the necessary transformations to the habitat dataset. # Reformat seagrassHabitat to merge with seagrassDensity ## replicate 0 - 5m transect dist to match with 0m in density survey; ## rest of habitat bins can map one to one with density (ie. 5 - 10m -&gt; 10m) seagrass0tmp &lt;- seagrassHabitat %&gt;% filter(transect_dist %in% c(&quot;0 - 5&quot;, &quot;0 - 2.5&quot;)) %&gt;% mutate(transect_dist = factor(0)) ## collapse various levels to match with seagrassDensity transect_dist seagrassHabitat$transect_dist &lt;- fct_collapse(seagrassHabitat$transect_dist, &quot;5&quot; = c(&quot;0 - 5&quot;, &quot;2.5 - 7.5&quot;), &quot;10&quot; = c(&quot;5 - 10&quot;, &quot;7.5 - 12.5&quot;), &quot;15&quot; = c(&quot;10 - 15&quot;, &quot;12.5 - 17.5&quot;), &quot;20&quot; = c(&quot;15 - 20&quot;, &quot;17.5 - 22.5&quot;), &quot;25&quot; = c(&quot;20 - 25&quot;, &quot;22.5 - 27.5&quot;), &quot;30&quot; = c(&quot;25 - 30&quot;, &quot;27.5 - 30&quot;)) ## merge seagrass0tmp into seagrassHabitat to account for 0m samples, ## set class for date, datetime variables seagrassHabitatFull &lt;- rbind(seagrass0tmp, seagrassHabitat) %&gt;% filter(transect_dist != &quot;0 - 2.5&quot;) %&gt;% # already captured in seagrass0tmp droplevels(.) # remove now unused factor levels With the distances of habitat and density surveys now corresponding, we can now merge these two datasets plus there coordinates together, combine redundant fields, and remove unnecessary fields. # Merge seagrassHabitatFull with seagrassDensity, then coordinates seagrass &lt;- full_join(seagrassHabitatFull, seagrassDensity, by = c(&quot;organization&quot;, &quot;work_area&quot;, &quot;project&quot;, &quot;survey&quot;, &quot;site_id&quot;, &quot;date&quot;, &quot;transect_dist&quot;)) %&gt;% # merge hakai_id.x and hakai_id.y into single variable field; # use combination of date, site_id, transect_dist, and field uid (hakai_id # when present) mutate(field_uid = ifelse(sample_collected == TRUE, hakai_id.x, &quot;NA&quot;), hakai_id = paste(date, &quot;HAKAI:CALVERT&quot;, site_id, transect_dist, sep = &quot;:&quot;), # below, aggregate metadata that didn&#39;t merge naturally (ie. due to minor # differences in watch time or depth gauges) dive_supervisor = dive_supervisor.x, collected_start = ymd_hms(ifelse(is.na(collected_start.x), collected_start.y, collected_start.x)), collected_end = ymd_hms(ifelse(is.na(collected_start.x), collected_start.y, collected_start.x)), depth_m = ifelse(is.na(depth.x), depth.y, depth.x), sampling_bout = sampling_bout.x) %&gt;% left_join(., coordinates, # add coordinates by = c(&quot;site_id&quot; = &quot;Point.Name&quot;)) %&gt;% select( - c(X.x, X.y, hakai_id.x, hakai_id.y, # remove unnecessary variables dive_supervisor.x, dive_supervisor.y, collected_start.x, collected_start.y, collected_end.x, collected_end.y, depth.x, depth.y, sampling_bout.x, sampling_bout.y)) %&gt;% mutate(density_msq = as.character(density_msq), canopy_height_cm = as.character(canopy_height_cm), flowering_shoots = as.character(flowering_shoots), depth_m = as.character(depth_m)) %T&gt;% glimpse() 2.2.2 Convert Data to Darwin Core - Extended Measurement or Fact format The Darwin Core ExtendedMeasurementOrFact (eMoF) extension bases records around a core event (rather than occurrence as in standard Darwin Core), allowing for additional measurement variables to be associated with occurrence data. 2.2.2.1 Add Event ID and Occurrence ID variables to dataset As this dataset will be annually updated, rather than using natural keys (ie. using package::uuid to autogenerate) for event and occurence IDs, here we will use surrogate keys made up of a concatenation of date survey, transect location, observation distance, and sample ID (for occurrenceID, when a sample is present). # create and populate eventID variable ## currently only event is used, but additional surveys and abiotic data ## are associated with parent events that may be included at a later date seagrass$eventID &lt;- seagrass$hakai_id # create and populate occurrenceID; combine eventID with transect_dist # and field_uid ## in the event of &lt;NA&gt; field_uid, no sample was collected, but ## measurements and occurrence are still taken; no further subsamples ## are associated with &lt;NA&gt; field_uids seagrass$occurrenceID &lt;- with(seagrass, paste(eventID, transect_dist, field_uid, sep = &quot;:&quot;)) 2.2.2.2 Create Event, Occurrence, and eMoF tables Now that we’ve created eventIDs and occurrenceIDs to connect all the variables together, we can begin to create the Event, Occurrence, and extended Measurement or Fact table necessary for DarwinCore compliant datasets 2.2.2.2.1 Event Table # subset seagrass to create event table seagrassEvent &lt;- seagrass %&gt;% distinct %&gt;% # some duplicates in data stemming from database conflicts select(date, Decimal.Lat, Decimal.Long, transect_dist, depth_m, eventID) %&gt;% rename(eventDate = date, decimalLatitude = Decimal.Lat, decimalLongitude = Decimal.Long, coordinateUncertaintyInMeters = transect_dist, minimumDepthInMeters = depth_m, maximumDepthInMeters = depth_m) %&gt;% mutate(geodeticDatum = &quot;WGS84&quot;, samplingEffort = &quot;30 metre transect&quot;) %T&gt;% glimpse # save event table to csv write.csv(seagrassEvent, &quot;../datasets/hakai_seagrass_data/processed_data/hakaiSeagrassDwcEvent.csv&quot;) 2.2.2.2.2 Occurrence Table # subset seagrass to create occurrence table seagrassOccurrence &lt;- seagrass %&gt;% distinct %&gt;% # some duplicates in data stemming from database conflicts select(eventID, occurrenceID) %&gt;% mutate(basisOfRecord = &quot;HumanObservation&quot;, scientificName = &quot;Zostera subg. Zostera marina&quot;, occurrenceStatus = &quot;present&quot;) # Taxonomic name matching # in addition to the above metadata, DarwinCore format requires further # taxonomic data that can be acquired through the WoRMS register. ## Load taxonomic info, downloaded via WoRMS tool # zmWorms &lt;- # read.delim(&quot;raw_data/zmworms_matched.txt&quot;, # header = TRUE, # nrows = 1) zmWorms &lt;- wm_record(id = 145795) # join WoRMS name with seagrassOccurrence create above seagrassOccurrence &lt;- full_join(seagrassOccurrence, zmWorms, by = c(&quot;scientificName&quot; = &quot;scientificname&quot;)) %&gt;% select(eventID, occurrenceID, basisOfRecord, scientificName, occurrenceStatus, AphiaID, url, authority, status, unacceptreason, taxonRankID, rank, valid_AphiaID, valid_name, valid_authority, parentNameUsageID, kingdom, phylum, class, order, family, genus, citation, lsid, isMarine, match_type, modified) %T&gt;% glimpse # save occurrence table to csv write.csv(seagrassOccurrence, &quot;../datasets/hakai_seagrass_data/processed_data/hakaiSeagrassDwcOccurrence.csv&quot;) 2.2.2.2.3 Extended MeasurementOrFact table seagrassMof &lt;- seagrass %&gt;% # select variables for eMoF table select(date, eventID, survey, site_id, transect_dist, substrate, patchiness, adj_habitat_1, adj_habitat_2, vegetation_1, vegetation_2, density_msq, canopy_height_cm, flowering_shoots) %&gt;% # split substrate into two variables (currently holds two substrate type in same variable) separate(substrate, sep = &quot;,&quot;, into = c(&quot;substrate_1&quot;, &quot;substrate_2&quot;)) %&gt;% # change variables names to match NERC database (or to be more descriptive where none exist) rename(measurementDeterminedDate = date, SubstrateTypeA = substrate_1, SubstrateTypeB = substrate_2, BarePatchLengthWithinSeagrass = patchiness, PrimaryAdjacentHabitat = adj_habitat_1, SecondaryAdjacentHabitat = adj_habitat_2, PrimaryAlgaeSp = vegetation_1, SecondaryAlgaeSp = vegetation_2, BedAbund = density_msq, CanopyHeight = canopy_height_cm, FloweringBedAbund = flowering_shoots) %&gt;% # reformat variables into DwC MeasurementOrFact format # (single values variable, with measurement type, unit, etc. variables) pivot_longer( - c(measurementDeterminedDate, eventID, survey, site_id, transect_dist), names_to = &quot;measurementType&quot;, values_to = &quot;measurementValue&quot;, values_ptypes = list(measurementValue = &quot;character&quot;)) %&gt;% # use measurement type to fill in remainder of variables relating to # NERC vocabulary and metadata fields mutate( measurementTypeID = case_when( measurementType == &quot;BedAbund&quot; ~ &quot;http://vocab.nerc.ac.uk/collection/P01/current/SDBIOL02/&quot;, measurementType == &quot;CanopyHeight&quot; ~ &quot;http://vocab.nerc.ac.uk/collection/P01/current/OBSMAXLX/&quot;, # measurementType == &quot;BarePatchWithinSeagrass&quot; ~ &quot;&quot;, measurementType == &quot;FloweringBedAbund&quot; ~ &quot;http://vocab.nerc.ac.uk/collection/P01/current/SDBIOL02/&quot;), measurementUnit = case_when( measurementType == &quot;BedAbund&quot; ~ &quot;Number per square metre&quot;, measurementType == &quot;CanopyHeight&quot; ~ &quot;Centimetres&quot;, measurementType == &quot;BarePatchhLengthWithinSeagrass&quot; ~ &quot;Metres&quot;, measurementType == &quot;FloweringBedAbund&quot; ~ &quot;Number per square metre&quot;), measurementUnitID = case_when( measurementType == &quot;BedAbund&quot; ~ &quot;http://vocab.nerc.ac.uk/collection/P06/current/UPMS/&quot;, measurementType == &quot;CanopyHeight&quot; ~ &quot;http://vocab.nerc.ac.uk/collection/P06/current/ULCM/&quot;, measurementType == &quot;BarePatchhLengthWithinSeagrass&quot; ~ &quot;http://vocab.nerc.ac.uk/collection/P06/current/ULAA/2/&quot;, measurementType == &quot;FloweringBedAbund&quot; ~ &quot;http://vocab.nerc.ac.uk/collection/P06/current/UPMS/&quot;), measurementAccuracy = case_when( measurementType == &quot;CanopyHeight&quot; ~ 5), measurementMethod = case_when( measurementType == &quot;BedAbund&quot; ~ &quot;25cmx25cm quadrat count&quot;, measurementType == &quot;CanopyHeight&quot; ~ &quot;in situ with ruler&quot;, measurementType == &quot;BarePatchhLengthWithinSeagrass&quot; ~ &quot;estimated along transect line&quot;, measurementType == &quot;FloweringBedAbund&quot; ~ &quot;25cmx25cm quadrat count&quot;)) %&gt;% select(eventID, measurementDeterminedDate, measurementType, measurementValue, measurementTypeID, measurementUnit, measurementUnitID, measurementAccuracy, measurementMethod) %T&gt;% # select(!c(survey, site_id, transect_dist)) %T&gt;% glimpse() # save eMoF table to csv write.csv(seagrassMof, &quot;../datasets/hakai_seagrass_data/processed_data/hakaiSeagrassDwcEmof.csv&quot;) 2.2.3 Session Info Print session information below in case necessary for future reference # Print Session Info for future reference sessionInfo() "],["final-words.html", "Chapter 3 Final Words", " Chapter 3 Final Words We have finished a nice book. "],["tools.html", "Chapter 4 Tools 4.1 R 4.2 Python 4.3 Google Sheets 4.4 Validators", " Chapter 4 Tools Below are some of the tools and packages used in workflows. R and Python package “Type” is BIO for packages specifically for biological applications, and GEN for generic packages. 4.1 R Package Type Description bdveRse BIO A family of R packages for biodiversity data. ecocomDP BIO Work with the Ecological Community Data Design Pattern. ‘ecocomDP’ is a flexible data model for harmonizing ecological community surveys, in a research question agnostic format, from source data published across repositories, and with methods that keep the derived data up-to-date as the underlying sources change. EDIorg/EMLasseblyline BIO For scientists and data managers to create high quality EML metadata for dataset publication. finch BIO Parse Darwin Core Files iobis/obistools BIO Tools for data enhancement and quality control. robis BIO R client for the OBIS API ropensci/EML BIO Provides support for the serializing and parsing of all low-level EML concepts taxize BIO Interacts with a suite of web ‘APIs’ for taxonomic tasks, such as getting database specific taxonomic identifiers, verifying species names, getting taxonomic hierarchies, fetching downstream and upstream taxonomic names, getting taxonomic synonyms, converting scientific to common names and vice versa, and more. worrms BIO Client for World Register of Marine Species. Includes functions for each of the API methods, including searching for names by name, date and common names, searching using external identifiers, fetching synonyms, as well as fetching taxonomic children and taxonomic classification. Hmisc GEN Contains many functions useful for data analysis, high-level graphics, utility operations, functions for computing sample size and power, simulation, importing and annotating datasets, imputing missing values, advanced table making, variable clustering, character string manipulation, conversion of R objects to LaTeX and html code, and recoding variables. Particularly check out the describe() function. lubridate GEN Functions to work with date-times and time-spans: fast and user friendly parsing of date-time data, extraction and updating of components of a date-time (years, months, days, hours, minutes, and seconds), algebraic manipulation on date-time and time-span objects. stringr GEN Simple, Consistent Wrappers for Common String Operations tidyverse GEN The ‘tidyverse’ is a set of packages that work in harmony because they share common data representations and ‘API’ design. This package is designed to make it easy to install and load multiple ‘tidyverse’ packages in a single step. uuid GEN Tools for generating and handling of UUIDs (Universally Unique Identifiers). 4.2 Python Package Type Description metapype BIO A lightweight Python 3 library for generating EML metadata python-dwca-reader BIO A simple Python package to read and parse Darwin Core Archive (DwC-A) files, as produced by the GBIF website, the IPT and many other biodiversity informatics tools. pyworms BIO Python client for the World Register of Marine Species (WoRMS) REST service. numpy GEN NumPy (Numerical Python) is an open source Python library that’s used in almost every field of science and engineering. It’s the universal standard for working with numerical data in Python, and it’s at the core of the scientific Python and PyData ecosystems. pandas GEN pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language. Super helpful when manipulating tabular data! uuid GEN This module provides immutable UUID objects (class UUID) and the functions uuid1(), uuid3(), uuid4(), uuid5() for generating version 1, 3, 4, and 5 UUIDs as specified in RFC 4122. Built in – part of the Python standard library. 4.3 Google Sheets Package Description Google Sheet DarwinCore Archive Assistant add-on Google Sheet add-on which assists the creation of Darwin Core Archives (DwCA) and publising to Zenodo. DwCA’s are stored into user’s Google Drive and can be downloaded for upload into IPT installations or other software which is able to read DwC-archives. 4.4 Validators Name Description Darwin Core Archive Validator This validator verifies the structural integrity of a Darwin Core Archive. It does not check the data values, such as coordinates, dates or scientific names. GBIF DATA VALIDATOR The GBIF data validator is a service that allows anyone with a GBIF-relevant dataset to receive a report on the syntactical correctness and the validity of the content contained within the dataset. LifeWatch Belgium Through this interactive section of the LifeWatch.be portal users can upload their own data using a standard data format, and choose from several web services, models and applications to process the data. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
